========================== How to check the configuration file?
=> All the files are present under hdfs which is Hadoop Distributed File System. so these files are not present in your filesystem or your directory structure
=> inside hdfs these are stored as: Path("hdfs://host:port/file"));
=>The setting of the port is present in your xml file under configuration directory of hadoop: $HADOOP_HOME/etc/hadoop/core-site.xml
=> check hadoop home
echo $HADOOP_HOME
=> for itversity, $HADOOP_HOME = /opt/hadoop
cat /opt/hadoop/etc/hadoop/core-site.xml
cd /opt/hadoop/etc/hive/conf

=> view hive-site.xml
=> In general, it will be present in
cat /etc/hive/conf/hive-site.xml
=> In itversity
cat /opt/hive/conf/hive-site.xml

========================== show column headers
set hive.cli.print.header=true;

========================== exclude headers/footers from loading into table
CREATE TABLE temp 
  ( 
     name STRING, 
     id   INT 
  ) 
row format delimited fields terminated BY '\t' lines terminated BY '\n' 
tblproperties('skip.header.line.count'='1'); 

=> To skip first 3 rows
tblproperties('skip.header.line.count'='3');

=> to skip last 3 rows
tblproperties('skip.footer.line.count'='3');

========================== To view the warehouse directory
hive
set hive.metastore.warehouse.dir;

========================== To create database
set hive.metastore.warehouse.dir=/user/itv736079/warehouse;

create database subhayang;

========================== Use beeline
beeline -u jdbc:hive2://

========================== Exit from beeline
!q

========================== Directly from terminal we can run an adhoc query

beeline -u jdbc:hive2:// -e "select * from subhayang.orders"

=> This will connect to beeline
=> run the query
=> exit from beeline

========================== how to run a script from terminal (normally in production scripts are run using a scheduler like Oozie or Airflow)

touch myscript.hql
=> file extension .hql is just for our own reference, in general linux does not care
cat > myscript.hql
show databases;
use subhayang;
Ctrl+D to save and exit

beeline -u jdbc:hive2:// -f /home/itv736079/myscript.hql

=> running a script from beeline itself (already logged into beeline)
source /home/itv736079/myscript.hql

========================== How to access HDFS from Hive terminal
dfs -ls /user/itv736079/warehouse/subhayang.db;

========================== How to check the table type
hive
describe formatted table_name;
=>Table Type: MANAGED TABLE

========================== Create a managed table

CREATE TABLE IF NOT EXISTS products_managed(
id STRING,
title STRING,
cost FLOAT
) 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

========================== Load data from LFS to managed table

LOAD DATA LOCAL INPATH '/home/itv736079/products.csv'
INTO TABLE products_managed;

=> notice the LOCAL, this means loading data from LFS
=> We are not specifying where the data should be kept
=> data will be stored in default path: /user/itv736079/warehouse/subhayang.db/products_managed
=> In this path you'll see the file: products.csv, so it got placed to HDFS from LFS

========================== Load data from HDFS to managed table

=> Create a directory in HDFS
hadoop fs -mkdir /user/itv736079/data

=> Load data to HDFS from LFS
hadoop fs -put /home/itv736079/products.csv /user/itv736079/data/

=> verify
hadoop fs -ls /user/itv736079/data/

=> Load data to table
hive
use subhayang;
LOAD DATA INPATH '/user/itv736079/data/products.csv'
INTO TABLE products_managed;

=> After this data will be moved to /user/itv736079/warehouse/subhayang.db/products_managed from /user/itv736079/data/
=> so we can not see products.csv in /user/itv736079/data/

=> check data in default path
hadoop fs -ls /user/itv736079/warehouse/subhayang.db/products_managed

========================== Overwrite the data

LOAD DATA LOCAL INPATH '/home/itv736079/products.csv'
OVERWRITE INTO TABLE products_managed;

========================== Table to table load

=> create another managed table
CREATE TABLE IF NOT EXISTS products_managed2(
id STRING,
title STRING,
cost FLOAT
) 
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

INSERT INTO products_managed2
SELECT * FROM products_managed;

========================== Create EXTERNAL table

=> /user/itv736079/data1/products.csv is the file location
CREATE EXTERNAL TABLE products(
id STRING,
title STRING,
cost FLOAT
) 
LOCATION '/user/itv736079/data1/';
=> If we load the data using above command, 2nd and 3rd column will be populated with null values
=> as the data is comma separated but we did not specify that

=> correct way to create would be
CREATE EXTERNAL TABLE products(
id STRING,
title STRING,
cost FLOAT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/user/itv736079/data1/';

=> data will be kept in the same HDFS path
=> i.e. data won't be moved to /user/itv736079/warehouse/subhayang.db/products from /user/itv736079/data1/
=> verify
hadoop fs -ls /user/itv736079/data1/

=======> What will happen if data is present in default warehouse directory and we create an external table?
=> keep data in default warehouse directory
hadoop fs -cp /user/itv736079/data1/products.csv /user/itv736079/warehouse/
=> create external table
CREATE EXTERNAL TABLE products_ext_def_dir(
id STRING,
title STRING,
cost FLOAT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/user/itv736079/warehouse/subhayang.db/products_ext/';
=> Now if we drop the table, data remains in the same place as the table was an external table.

========================== Create a table with an array type column
CREATE TABLE mobile_phones(
id STRING,
title STRING,
cost FLOAT,
colours ARRAY<string>,
screen_size ARRAY<float>
);

CREATE TABLE mobile_phones(
id STRING,
title STRING,
cost FLOAT,
colours ARRAY<string>,
screen_size ARRAY<float>
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY '#';

========================== Create a table with a map type column
CREATE TABLE mobile_phones(
id STRING,
title STRING,
cost FLOAT,
colours ARRAY<string>,
screen_size ARRAY<float>,
features MAP<string, boolean>
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY '#'
MAP KEYS TERMINATED BY ':';


========================== Create a table with a struct type column
CREATE TABLE mobile_phones(
id STRING,
title STRING,
cost FLOAT,
colours ARRAY<string>,
screen_size ARRAY<float>,
features MAP<string, boolean>,
information struct<battery:string,camera:string>
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY '#'
MAP KEYS TERMINATED BY ':';

========================== UDF
CREATE TABLE IF NOT EXISTS sample_table(
name STRING,
count INT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';

LOAD DATA LOCAL INPATH '/home/itv736079/sample_data.txt'
INTO TABLE sample_table;

=> temporary function (session specific)
=> Add jar
hive
ADD jar /home/itv736079/udf_hive_touppercase.jar;

=> create a temporary function
CREATE TEMPORARY FUNCTION touppr 
	AS 'udf_hive_touppercase.ToUpperCase';
=> package_name.class_name

=> Permanent function
=> Move jar to HDFS
hadoop fs -mkdir /user/itv736079/hivejars
hadoop fs -put /home/itv736079/udf_hive_touppercase.jar /user/itv736079/hivejars
=> Create permanent function
CREATE FUNCTION touppr 
AS 'udf_hive_touppercase.ToUpperCase' USING jar
'hdfs://m01.itversity.com:9000/user/itv736079/hivejars/udf_hive_touppercase.jar';

========================== How to skip the first line of csv while loading in hive table
CREATE TABLE temp 
  ( 
     name STRING, 
     id   INT 
  ) 
row format delimited fields terminated BY '\t' lines terminated BY '\n' 
tblproperties("skip.header.line.count"="1");

========================== Partitioning



========================== Bucketing
CREATE external TABLE customers_bucketed(
customer_id int,
customer_fname string,
customer_lname string,
customer_email string,
customer_password string,
customer_street string,
customer_city string,
customer_state string,
customer_zipcode string
)
clustered by(customer_id) into 4 buckets
row format delimited
fields terminated BY ',';

insert into customers_bucketed select * from customers;

=> validate
hadoop fs -ls /user/itv736079/warehouse/subhayang.db/customers_bucketed/

========================== Partitioning with Bucketing



========================== Map side join
=> Automatic map side join if possible
set hive.auto.convert.join=true;
set hive.mapjoin.smalltable.filesize;

======================================= Inner join as map side join using hints
=> By default, the below property will be set to true. We need to set it to false to allow hints.
set hive.ignore.mapjoin.hint=false;

=> And also, the below property will have to be set to false so that Hive does not automatically convert the joins to MapJoin.
set hive.auto.convert.join=false;

SELECT /*+ MAPJOIN(c) */
c.customer_id,
c.customer_fname,
c.customer_lname,
o.order_id,
o.order_date
FROM orders o
JOIN customers c
ON (o.order_customer_id = c.customer_id)
limit 5;
=> Here o is the small table/ broadcast table

=> small table
set hive.mapjoin.smalltable.filesize;


=================================== Bucket Map Join
=> Enforce bucketing
set hive.enforce.bucketing=true;
SET hive.optimize.bucketmapjoin = true;
=> enable Map Join
set hive.auto.convert.join=true;

CREATE external TABLE customers_bucketed(
customer_id int,
customer_fname string,
customer_lname string,
customer_email string,
customer_password string,
customer_street string,
customer_city string,
customer_state string,
customer_zipcode string
)
clustered by(customer_id) into 4 buckets
row format delimited
fields terminated BY ',';

insert into customers_bucketed select * from customers;

CREATE external TABLE orders_bucketed(
order_id int,
order_date string,
order_customer_id int,
order_status string
)
clustered by(order_customer_id) into 8 buckets
row format delimited
fields terminated BY ',';

insert into orders_bucketed select * from orders;

=> validate, as we did not specify the location while creating the external table, directory will be default
hadoop fs -ls /user/itv736079/warehouse/subhayang.db/customers_bucketed/
hadoop fs -ls /user/itv736079/warehouse/subhayang.db/orders_bucketed/



SELECT c.customer_id, c.customer_fname, c.customer_lname, o.order_id, o.order_date FROM customers_bucketed c JOIN orders_bucketed o ON (c.customer_id = o.order_customer_id) limit 10;


=================================== Sort Merge Bucket Join
set hive.auto.convert.sortmerge.join=true;
set hive.auto.convert.sortmerge.join.noconditionaltask=true;
set hive.optimize.bucketmapjoin=true;
set hive.optimize.bucketmapjoin.sortedmerge=true;
set hive.enforce.bucketing=true;
set hive.enforce.sorting=true;
set hive.auto.convert.join=true;

CREATE TABLE customers_bucketed(
customer_id int,
customer_fname string,
customer_lname string,
customer_email string,
customer_password string,
customer_street string,
customer_city string,
customer_state string,
customer_zipcode string
)
clustered by(customer_id)
sorted by(customer_id asc) into 4 buckets
row format delimited
fields terminated BY ',';

insert into customers_bucketed select * from customers;

CREATE TABLE orders_bucketed(
order_id int,
order_date string,
order_customer_id int,
order_status string
)
clusteredby(order_customer_id)
sorted by(order_customer_id asc) into 4 buckets
row format delimited
fields terminated BY ',';

insert into orders_bucketed select * from orders;

SELECT c.customer_id, c.customer_fname, c.customer_lname, o.order_id, o.order_date FROM customers_bucketed c JOIN orders_bucketed o ON (c.customer_id = o.order_customer_id) limit 10;

=> To validate if SMB has occurred or not
EXPLAIN EXTENDED
SELECT c.customer_id, c.customer_fname, c.customer_lname, o.order_id, o.order_date FROM customers_bucketed c JOIN orders_bucketed o ON (c.customer_id = o.order_customer_id) limit 10;


================================================ tblproperties
=> block size
set dfs.block.size;

=> default file format
set hive.default.fileformat;

=> strict/ non-strict, in strict mode, while using order by clause we need to use limit also
set hive.mapred.mode;

=> using number in place of column names in order by clause 
set hive.groupby.orderby.position.alias;

=> determine number of reducers
set mapred.reduce.tasks;
set hive.exec.reducers.bytes.per.reducer;

================================================= Hive variables (hiveconf, hivevar)
=> hiveconf
set department_number='40’;
set hiveconf d1='20'; 

SELCET * FROM employee_table WHERE column_6=${hiveconf:department_number};

=> hivevar
set hivevar:dept_num= 10;
SELECT * FROM emp_tab WHERE col6=${dept_num};

=> run a hive query from shell using variables
hive --hiveconf dept_no -e 'select * from emp_tab where col6=${hiveconf:department_number}'

=> variables and hql scripts
hive --hivevar emp_id=col1 --hiveconf table_name=emp_tab --hivevar dept_no=10 -f /home/itv736079/myscript/variables.hql

=> substituting variables
set hive.variable.substitute=true;
set table=table9;
set new_table={hiveconf:table};
set new_table;
=> It will show new_table=table9 
select * from ${hiveconf:new_table};
=> will return values from table9

================================================== check value of all Hive and Hadoop configuration variables
set -v
